# Transformer_Relative_Position_Self_Attention
Pytorch implementation of the paper ["Self-Attention with Relative Position Representations"](https://arxiv.org/pdf/1803.02155.pdf) \
For the entire Seq2Seq framework, you can refer to this [repo](https://github.com/evelinehong/hint-baselines/tree/master/cnn-transformer-relative-position).
